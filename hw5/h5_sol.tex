\documentclass[a4paper,11pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage[headsepline]{scrlayer-scrpage}
\ihead{Bernd Schwarzenbacher}
\chead{CMSDE HW5}
\ohead{\today}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{commath}
\usepackage{mathtools}

\usepackage{hyperref}
\usepackage{listings}
\lstset{language=Python,
  frame=single,
  breaklines=true,
  captionpos=b
}

\newcommand*{\R}{\mathbb{R}}
\newcommand*{\EV}[1]{\mathbb{E}\left[{#1}\right]}
\newcommand*{\bt}{\bold{\theta}}

\begin{document}

\begin{enumerate}

\item
  Running the code, I get an accuracy of $0.9455$.

\item
  The accuracy turned out to be worse: $0.938$.

\item
  With the Adam optimizer, I get an accuracy of $0.9423$.
  Doubling the number of iterations $M$ to $10000$ we finally get an improved
  accuracy of $0.9503$.

\item
  The new optimization problem is
  \[ \min_{\bt \in \R^{((784 + 1) + 2 (K+1) + 10) \times K}} \EV{H(y(X), S(\alpha_\bt(X)))}\]
  for the random handwritten digit $X \in \R^{784}$ with the corresponding
  one-hot label $y(X)$.
  $\alpha_\bt(x)$ is the neural network with 3 hidden layers,
  where each hidden layer has $K$ units plus a bias unit.
  It can be written as:
  \[ \alpha_\bt(x) = \bold{\Theta_4} \cdot r(\bold{\Theta_3} \cdot
    r(\bold{\Theta_2} \cdot r(\bold{\Theta_1} \cdot x + \bold{\theta_1}) + \bold{\theta_2})
    + \bold{\theta_3}), \; x \in \R^{784} \]
  \[ \bold{\Theta_1} \in \R^{K \times 784}, \bold{\Theta_2} \in \R^{K \times
      K}, \bold{\Theta_3} \in \R^{K \times K}, \bold{\Theta_4} \in \R^{10 \times
    K}\]
\[ \bold{\theta_1} \in \R^K, \bold{\theta_2} \in \R^K, \bold{\theta_2} \in \R^K\]

  with the ReLU activation unit (vectorized for the above neural network notation):
  \[ r(x) = \max(0, x)\]
  We still use the cross-entropy function
  \[H(\bold{y}, \bold{z}) = -\sum^{10}_{i=1}y_i \log(z_i), \quad \bold{y}, \bold{z}
    \in \R^{10} \]
  and the softmax function
  \[ S(\bold{z}) = \left(
      \frac{e^{z_1}}{\sum^{10}_{i=1} e^{z_i}},
      \frac{e^{z_2}}{\sum^{10}_{i=1} e^{z_i}},
      \dots
      \frac{e^{z_10}}{\sum^{10}_{i=1} e^{z_i}},
    \right), \quad \bold{z} \in \R^{10}\]

  The accuracy only improved a little bit to $0.9559$.


\item
Increasing the number of nodes per layer $K$ to $600$ increased the accuracy to $0.9661$.

\end{enumerate}

\section*{Code Appendix}

The final code can also be found online at\\
\url{https://github.com/bschwb/cmsde/tree/master/hw5}

\lstset{caption={Final code}, label=lstex1b}
\lstinputlisting{tf_mnist_lab.py}

\end{document}