\documentclass[a4paper,11pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage[headsepline]{scrlayer-scrpage}
\ihead{Bernd Schwarzenbacher}
\chead{CMSDE HW4}
\ohead{\today}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{commath}
\usepackage{mathtools}
\usepackage[retainorgcmds]{IEEEtrantools}

\usepackage{hyperref}
\usepackage[noabbrev]{cleveref}
\usepackage{float}
\usepackage{graphicx}
\usepackage{listings}
\lstset{language=Python,
  frame=single,
  breaklines=true,
  captionpos=b
}

\newcommand*{\R}{\mathbb{R}}
\newcommand*{\EV}[1]{\mathbb{E}\left[{#1}\right]}
\newcommand*{\Dt}{\Delta{}t}
\newcommand*{\DW}{\Delta{}W}
\newcommand*{\fdt}{\frac{\partial{}f}{\partial{}\theta}}

\usepackage{enumitem}

\begin{document}

\begin{enumerate}

\item

\begin{enumerate}[leftmargin=1em]
  \item
    To obtain the a bound for the iterated parameters I first note that
    \[f(\theta,y) \coloneqq (\theta - y)^2 \Rightarrow
      \fdt = 2 (\theta - y) \]

    which is used to compute the following.
      \begin{IEEEeqnarray*}{rCl}
        \EV{\theta_{n+1}^2} &=& \EV{\left(\theta_n - \Dt
            \fdt(\theta_n, Y_n)\right)^2}
        = \EV{\left(\theta_n-2\Dt(\theta_n-Y_n)\right)^2}\\
        &=& \EV{\theta_{n}^2} - 4 \Dt \EV{\theta_n(\theta_n-Y_n)} + 4 \Dt^2
        \EV{(\theta_n-Y_n)^2} \\
        &=& \EV{\theta_n^2} - 4\Dt\EV{\theta_n^2} + 4 \Dt \EV{\theta_n Y_n} +
        4\Dt^2\EV{\theta_n^2} - 8\Dt^2\EV{\theta_n Y_n} + 4 \Dt^2 \EV{Y_n^2} \\
        &=& \EV{\theta_n^2} (1 - 4\Dt + 4\Dt^2) + 4\Dt^2 = \EV{\theta_n^2} (1 -
        2\Dt)^2 + 4\Dt^2 \\
        &=& \EV{\theta_0^2} (1-2\Dt)^{2(n+1)} + 4 \Dt^2 \sum^{n}_{i=0}
        (1-2\Dt)^{2k}
      \end{IEEEeqnarray*}
      For the next steps to be valid, I need $0 < \Dt < 0.5$ or $0.5 < \Dt < 1$.
      \begin{IEEEeqnarray*}{rCl}
        &=& \EV{\theta_0^2} (1-2\Dt)^{2(n+1)} + 4 \Dt^2 \frac{1 -
          (1-2\Dt)^{2n}}{1 - (1 - 2\Dt)^2} \\
        &\leq& \EV{\theta_0^2} + 4\Dt^2
      \end{IEEEeqnarray*}

    With $\theta_0 = 1$ I can use $C \geq 5$ as an upper bound for all $\Dt \in
    (0, 0.5) \cup (0.5, 1)$.

    To obtain $\theta_*$, I compute
    \[\EV{f(\theta,Y)} = \EV{\left(\theta-Y\right)^2} = \EV{\theta^2} -
      2\EV{\theta Y} + \EV{Y^2} = \theta^2 + 1\]
    This expression obtains its minimum for $\theta_* = 0$.
    So we can use the previous computation for the limit and get
    \begin{IEEEeqnarray*}{rCl}
    \underset{n\rightarrow\infty}\lim\EV{\left( \theta_n - \theta_* \right)^2}
      &=& \underset{n\rightarrow\infty}\lim\EV{(\theta_n)^2} \\
      &=& \underset{n\rightarrow\infty}\lim \EV{\theta_0^2}(1-2\Dt)^{2n} +
      \frac{4\Dt^2}{4\Dt - 4\Dt^2} (1 - (1 - 2\Dt)^{2n}) \\
      &=&  \frac{\Dt}{1 - \Dt}
    \end{IEEEeqnarray*}

    And the convergence rate is
    \[ \EV{\left(\theta_n - \theta_* \right)^2} - \frac{\Dt}{1 - \Dt}
    = \left(1 + \frac{\Dt}{1-\Dt}\right) (1-2\Dt)^{2n}\]

    \item
      The code can be found in listing~\ref{lstex1b} and the exponential
      convergence for $\theta$ is plotted in \cref{fig:theta_conv}.

      \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{pic/theta_conv.pdf}
        \caption{$\theta$ convergence for $\Dt = 0.0001$}
        \label{fig:theta_conv}
      \end{figure}

    \item
      The Ornstein-Uhlenbeck process is defined by the SDE
      \[ \dif{}X_t = \alpha(\mu-X_t)\dif{}s+\sigma\dif{}W, \quad X_0 = 1 \]
      The forward Euler approximation for it
      \[ X_{n+1} - X_n = \alpha(\mu - X_n) \Dt + \sigma \DW_n \]
      in comparison to the $\theta$ iteration
      \[\theta_{n+1} - \theta_n = 2 \Dt \theta_n - 2 \sqrt{\Dt} \DW_n\]
      with $\DW_n = \sqrt{\Dt} Y_n$ leads us to identify
      $\alpha = 2, \mu = 0, \sigma = 2 \sqrt{\Dt}$.

      The gradient descent method looks like the following:
      \begin{IEEEeqnarray*}{rCl}
      \theta_{n+1} &=& \theta_n - \Dt \frac{\partial}{\partial{}\theta} \EV{(\theta_n - Y)^2}
        = \theta_n - \Dt \frac{\partial}{\partial{}\theta}\left( \EV{\theta_n^2}
        - 2 \EV{\theta_n Y} + \EV{Y^2}\right) \\
      &=& \theta_n - \Dt \frac{\partial}{\partial{}\theta}\left(\theta_n^2 + 1\right)
      = \theta_n - 2\Dt \theta_n = \theta_0 (1 - 2\Dt)^{n+1}
      \end{IEEEeqnarray*}
      So convergence rate for it is similar to the stochastic gradient method,
      but it will convergence against the correct $\theta_* = 0$ instead of
      against $\frac{\Dt}{1 - \Dt}$.
\end{enumerate}

\item

\begin{enumerate}[leftmargin=1em]
  \item
    \begin{figure}[H]
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/learned_fun_5e-02.pdf}
          \caption{Learned function $\Dt=0.05$}
          \label{fig:learned_fun_0.05}
        \end{minipage}%
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/test_error_5e-02.pdf}
          \caption{Test error $\Dt=0.05$}
          \label{fig:test_error_0.05}
        \end{minipage}
    \end{figure}
    \begin{figure}[H]
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/learned_fun_5e-03.pdf}
          \caption{Learned function $\Dt=0.005$}
          \label{fig:learned_fun_0.005}
        \end{minipage}%
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/test_error_5e-03.pdf}
          \caption{Test error $\Dt=0.005$}
          \label{fig:test_error_0.005}
        \end{minipage}
    \end{figure}
    \begin{figure}[H]
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/learned_fun_5e-04.pdf}
          \caption{Learned function $\Dt=0.0005$}
          \label{fig:learned_fun_0.0005}
        \end{minipage}%
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/test_error_5e-04.pdf}
          \caption{Test error $\Dt=0.0005$}
          \label{fig:test_error_0.0005}
        \end{minipage}
    \end{figure}

  \item
    \begin{figure}[H]
      \centering
      \includegraphics[width=\linewidth]{pic/exp_vs_emp_loss.pdf}
      \caption{Empirical loss function $E_a$ and expected loss function $E_1$
        for $\Dt = 0.005$}
      \label{fig:empirical_loss}
    \end{figure}

  \item
    There is fast convergence for $K=1$ (see \cref{fig:test_error_K1}) since we
    only have a few parameters to train.
    The function fit is bad, since there are only enough parameters to fit a single
    sigmoid function and thus the quadratic function can not be approximated (\cref{fig:learned_fun_K1}).

    \begin{figure}[H]
        \begin{minipage}[t]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/learned_fun_K1.pdf}
          \caption{Learned function $\Dt=0.005, K=1$}
          \label{fig:learned_fun_K1}
        \end{minipage}%
        \begin{minipage}[t]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/test_error_K1.pdf}
          \caption{Test error $\Dt=0.005, K=1$}
          \label{fig:test_error_K1}
        \end{minipage}
    \end{figure}

   \item
     In \cref{fig:test_error_K1} we observe, that we need around 10 samples for
     the single node.
     Assuming this scales linearly, we need in the order of 10 samples per node.
     So the 100 samples for 10 nodes seem to be the right order of magnitude.

   \item
     To obtain a small value for the empirical cost function, but a large value
     for the expected cost function, the model needs to overfit.
     This means to make the model highly dependent on the training data, but bad
     to generalize.
     We can achieve this with a number of samples $N$ in order of the number of
     nodes $K$.
     In \cref{fig:exp_vs_emp_overfit} we see a big difference between the
     empirical and expected loss function.
     The model is overfitted.

    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{pic/exp_vs_emp_loss_overfit.pdf}
        \caption{Expected and empirical loss functions for $N=K=10$}
        \label{fig:exp_vs_emp_overfit}
    \end{figure}

   \item
     With a bias between $20\pi$ and $22\pi$, the sigmoid activation function is
     only evaluated at the top flat section, so there is no distinction in
     values, and nothing can be approximated. (see \cref{fig:learned_fun_bias})
     For the sinus activation function, the bias shift doesn't matter, since it is
     periodic and has no flat sections.
     So a decent function fit is obtained, as seen in \cref{fig:learned_fun_sin}.

    \begin{figure}[H]
        \begin{minipage}[t]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/learned_fun_bias.pdf}
          \caption{Learned function $\Dt=0.005$ with bias}
          \label{fig:learned_fun_bias}
        \end{minipage}%
        \begin{minipage}[t]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/test_error_bias.pdf}
          \caption{Test error $\Dt=0.005$ with bias}
          \label{fig:test_error_bias}
        \end{minipage}
    \end{figure}

    \begin{figure}[H]
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/learned_fun_sin.pdf}
          \caption{Learned function $\Dt=0.005$ with sin activation function}
          \label{fig:learned_fun_sin}
        \end{minipage}%
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/test_error_sin.pdf}
          \caption{Test error $\Dt=0.005$ with sin activation function}
          \label{fig:test_error_sin}
        \end{minipage}
    \end{figure}

    \item
     \[\min_{\theta\in\Theta} \EV{\left( \alpha_\theta(X) - f(X) \right)^2}\]
     \[f(x) = \left(x-\frac{1}{2}\right)^T\left(x-\frac{1}{2}\right), \quad
      x \in G \subset \R^d\]
      where $X \sim \mathcal{U}(G)$.
      \[\alpha_\theta(x) = \sum^K_{k=1}\phi_k^1\sigma\left( x^T \theta^2_k +
          \theta^3_k \right), \quad x \in G, \quad
        \theta = \left(\theta^1_k, \theta^2_k, \theta^3_k\right) \in \Theta = \R^K \times
        \R^{d\times K} \times \R^K\]
      with some activation function $\sigma(x)$.

    \begin{figure}[H]
      \centering
      \includegraphics[width=\linewidth]{pic/test_error_3dim.pdf}
      \caption{Test error 3-dimensional}
      \label{fig:multidim}
    \end{figure}
\end{enumerate}

\end{enumerate}
 
\section*{Code Appendix}

All code can be found online at
\url{https://github.com/bschwb/cmsde/tree/master/hw4}

\lstset{caption={Code for 1b}, label=lstex1b}
\lstinputlisting{exercise1b.py}

\lstset{caption={Code for 2g}, label=lstex2g}
\lstinputlisting{exercise2g.py}

\end{document}