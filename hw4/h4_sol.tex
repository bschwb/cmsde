\documentclass[a4paper,11pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage[headsepline]{scrlayer-scrpage}
\ihead{Bernd Schwarzenbacher}
\chead{CMSDE HW4}
\ohead{\today}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{commath}
\usepackage{mathtools}
\usepackage[retainorgcmds]{IEEEtrantools}

\usepackage[noabbrev]{cleveref}
\usepackage{graphicx}

\newcommand*{\EV}[1]{\mathbb{E}\left[{#1}\right]}
\newcommand*{\Dt}{\Delta{}t}
\newcommand*{\DW}{\Delta{}W}
\newcommand*{\fdt}{\frac{\partial{}f}{\partial{}\theta}}

\usepackage{enumitem}

\begin{document}

\begin{enumerate}

\item

\begin{enumerate}[leftmargin=1em]
  \item
    \[f(\theta,y) \coloneqq (\theta - y)^2 \Rightarrow
      \fdt = 2 (\theta - y) \]

    I prove by induction:
    \begin{itemize}
      \item \[\EV{\theta_0^2} = 1 \leq C\]

      \item
        \begin{IEEEeqnarray*}{rCl}
          \EV{\theta_{n+1}^2} &=& \EV{\left(\theta_n - \Dt
              \fdt(\theta_n, Y_n)\right)^2}
          = \EV{\left(\theta_n-2\Dt(\theta_n-Y_n)\right)^2}\\
          &=& \EV{\theta_{n}^2} - 4 \Dt \EV{\theta_n(\theta_n-Y_n)} + 4 \Dt^2
          \EV{(\theta_n-Y_n)^2} \\
          &=& \EV{\theta_n^2} - 4\Dt\EV{\theta_n^2} + 4 \Dt \EV{\theta_n Y_n} +
          4\Dt^2\EV{\theta_n^2} - 8\Dt^2\EV{\theta_n Y_n} + 4 \Dt^2 \EV{Y_n^2} \\
          &=& \EV{\theta_n^2} (1 - 4\Dt + 4\Dt^2) + 4\Dt^2 \leq C ( 1-4\Dt+4\Dt^2) +
          4 \Dt^2
        \end{IEEEeqnarray*}

        The last expression should be smaller than $C$:
        \begin{IEEEeqnarray*}{rCl}
          C(1 - 4\Dt + 4\Dt^2) + 4\Dt^2 &\leq& C \\
          C(-4\Dt + 4\Dt^2) + 4\Dt^2 &\leq& 0 \\
          \frac{4\Dt^2}{4\Dt - 4\Dt^2} &\leq& C \\
          \frac{\Dt}{1 - \Dt} &\leq& C
        \end{IEEEeqnarray*}
    \end{itemize}

    \[\EV{f(\theta,Y)} = \EV{\left(\theta-Y\right)^2} = \EV{\theta^2} -
      2\EV{\theta Y} + \EV{Y^2} = \theta^2 + 1\]
    is smallest for $\theta = 0$.

    No convergence? Looking at the plot, we see some convergence rate, but then
    oscillating within some bound around 0.

    \item
      \begin{figure}[h]
        \centering
        \includegraphics[width=\linewidth]{pic/theta_conv.pdf}
        \caption{$\theta$ convergence}
        \label{fig:theta_conv}
      \end{figure}

    \item
      Ornstein-Uhlenbeck process
      \[ \dif{}X_t = \alpha(\mu-X_t)\dif{}s+\sigma\dif{}W \]
      Forward Euler:
      \[ X_{n+1} - X_n = \alpha(\mu - X_n) \Dt + \sigma \DW_n \]

      \[\theta_{n+1} - \theta_n = 2 \Dt \theta_n - 2 \sqrt{\Dt} \DW_n)\]

      Since $\DW_n = \sqrt{\Dt} Y_n$
      $\alpha = 2, \mu = 0, \sigma = 2 \sqrt{\Dt}$

      Is it okay, that $\Dt$ appears in the definition of $\sigma$?
      It seems weird, because then the forward Euler method approximates a
      different process for a different $\Dt$.

      Gradient descent:
      \begin{IEEEeqnarray*}{rCl}
      \theta_{n+1} &=& \theta_n - \Dt \frac{\partial}{\partial{}\theta} \EV{(\theta_n - Y)^2}
        = \theta_n - \Dt \frac{\partial}{\partial{}\theta}\left( \EV{\theta_n^2}
        - 2 \EV{\theta_n Y} + \EV{Y^2}\right) \\
      &=& \theta_n - \Dt \frac{\partial}{\partial{}\theta}\left(\theta_n^2 + 1\right)
      = \theta_n - 2\Dt \theta_n
      \end{IEEEeqnarray*}

      Convergence rate:
      \begin{IEEEeqnarray*}{rCl}
      \EV{\theta_{n}^2} &=& \EV{\theta_{n-1}^2} - 4\Dt\EV{\theta_{n-1}^2} +
      4\Dt^2\EV{\theta_{n-1}^2} \\
      &=& \EV{\theta_{n-1}^2} (1 - 4\Dt + 4\Dt^2) \\
      &=& \EV{\theta_0^2} (1 - 4\Dt + 4\Dt^2)^n
      \underset{n\rightarrow\infty}{\longrightarrow} 0
      \end{IEEEeqnarray*}

      So convergence rate is $O\left((1-\Dt)^n\right)$?
\end{enumerate}

\item

\begin{enumerate}[leftmargin=1em]
  \item
    \begin{figure}[h]
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/learned_fun_5e-02.pdf}
          \caption{Learned function $\Dt=0.05$}
          \label{fig:learned_fun_0.05}
        \end{minipage}%
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/test_error_5e-02.pdf}
          \caption{Test error $\Dt=0.05$}
          \label{fig:test_error_0.05}
        \end{minipage}
    \end{figure}
    \begin{figure}[h]
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/learned_fun_5e-03.pdf}
          \caption{Learned function $\Dt=0.005$}
          \label{fig:learned_fun_0.005}
        \end{minipage}%
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/test_error_5e-03.pdf}
          \caption{Test error $\Dt=0.005$}
          \label{fig:test_error_0.005}
        \end{minipage}
    \end{figure}
    \begin{figure}[h]
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/learned_fun_5e-04.pdf}
          \caption{Learned function $\Dt=0.0005$}
          \label{fig:learned_fun_0.0005}
        \end{minipage}%
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/test_error_5e-04.pdf}
          \caption{Test error $\Dt=0.0005$}
          \label{fig:test_error_0.0005}
        \end{minipage}
    \end{figure}

  \item
    How is the empirical loss function different from the already plotted
    expected loss function?

  \item $K=1$
    I observe faster convergence in \cref{fig:test_error_K1} in comparison to \cref{fig:test_error_0.005}.
    \begin{figure}[h]
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/learned_fun_K1.pdf}
          \caption{Learned function $\Dt=0.005, K=1$}
          \label{fig:learned_fun_K1}
        \end{minipage}%
        \begin{minipage}[b]{.5\linewidth}
          \centering
          \includegraphics[width=\linewidth]{pic/test_error_K1.pdf}
          \caption{Test error $\Dt=0.005, K=1$}
          \label{fig:test_error_K1}
        \end{minipage}
    \end{figure}

\end{enumerate}

\end{enumerate}

\end{document}